{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYuV6JT4C5mi4laAHf5rVG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amanuel94/kaggle/blob/main/movie_review_tfds.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l8YqVscsxtu0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "import requests\n",
        "import zipfile\n",
        "import tarfile\n",
        "import random\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DOWNLOAD_DIR = \"./data\"\n",
        "DATA_URL = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n"
      ],
      "metadata": {
        "id": "c7sQek6Ucc-h"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz), which contains 50,000 movies reviews from the [Internet Movie Database](https://www.imdb.com/). The data is organized in two directories, train and test, each containing a pos subdirectory with 12,500 positive reviews and a neg subdirectory with 12,500 negative reviews. Each review is stored in a\n",
        "separate text file."
      ],
      "metadata": {
        "id": "08qigYcW2VOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_file(url, download_dir = DOWNLOAD_DIR):\n",
        "\n",
        "  data_path = pathlib.Path(download_dir)\n",
        "  if data_path.exists() and data_path.is_dir():\n",
        "        shutil.rmtree(data_path)\n",
        "        print(f'Removed existing directory: {data_path}')\n",
        "\n",
        "  file_name = url.split('/')[-1]\n",
        "  file_path = os.path.join(download_dir, file_name)\n",
        "  os.makedirs(download_dir, exist_ok=True)\n",
        "  response = requests.get(url)\n",
        "  response.raise_for_status()\n",
        "  with open(file_path, 'wb') as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "  if file_path.endswith('.zip'):\n",
        "        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(download_dir)\n",
        "            print(f'Extracted files to {download_dir}')\n",
        "  elif file_path.endswith('.tar.gz') or file_path.endswith('.tgz'):\n",
        "      with tarfile.open(file_path, 'r:gz') as tar_ref:\n",
        "          tar_ref.extractall(download_dir)\n",
        "          print(f'Extracted files to {download_dir}')\n",
        "\n",
        "  train_path = f\"{download_dir}/{file_name.split('_')[0]}/train/\"\n",
        "  test_path = f\"{download_dir}/{file_name.split('_')[0]}/test/\"\n",
        "\n",
        "  return train_path, test_path"
      ],
      "metadata": {
        "id": "8e5bgKO72qmR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainPath, testPath = get_file(DATA_URL, DOWNLOAD_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOO_lhfTelOh",
        "outputId": "b641ae37-4a8d-46a6-bffc-d3d6ce2c7857"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed existing directory: data\n",
            "Extracted files to ./data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_instances(dir_path):\n",
        "  dir_path = pathlib.Path(dir_path)\n",
        "  file_count = sum(1 for item in dir_path.iterdir() if item.is_file())\n",
        "  return file_count\n",
        "\n",
        "print(f\"Positive train instances: {count_instances(trainPath + 'pos/')}\")\n",
        "print(f\"Negative train instances: {count_instances(trainPath + 'neg/')}\")\n",
        "print(f\"Positive test instances: {count_instances(testPath + 'pos/')}\")\n",
        "print(f\"Negative test instances: {count_instances(testPath + 'neg/')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_pI8clhjzhf",
        "outputId": "599b8f4f-2ccd-4643-e5d9-69a65baa2c40"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive train instances: 12500\n",
            "Negative train instances: 12500\n",
            "Positive test instances: 12500\n",
            "Negative test instances: 12500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the test set into a validation set (15,000) and a test set (10,000).\n"
      ],
      "metadata": {
        "id": "AL_MNO3v2zza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def copy_files(sources, dest_dir):\n",
        "\n",
        "  dest_path = pathlib.Path(dest_dir)\n",
        "\n",
        "  if dest_path.exists() and dest_path.is_dir():\n",
        "        shutil.rmtree(dest_path)\n",
        "        print(f'Removed existing directory: {dest_path}')\n",
        "\n",
        "  dest_path.mkdir(parents = True, exist_ok = True)\n",
        "\n",
        "  for file_path in sources:\n",
        "    shutil.copy(file_path, dest_path / file_path.name)\n",
        "\n",
        "\n",
        "def partition_files(file_path, n):\n",
        "  filePath = pathlib.Path(file_path)\n",
        "  all_files = [item for item in filePath.iterdir() if item.is_file()]\n",
        "  random.shuffle(all_files)\n",
        "  return all_files[:n], all_files[n:]\n",
        "\n",
        "\n",
        "\n",
        "def val_test_split(dir_path, val_size):\n",
        "\n",
        "  val_pos, test_pos = partition_files(dir_path + 'pos/', val_size//2)\n",
        "  val_neg, test_neg = partition_files(dir_path + 'neg/', val_size//2)\n",
        "\n",
        "  if dir_path[-1] == '/': dir_path = dir_path[:-1]\n",
        "  root_path = dir_path[:dir_path.rindex('/')]\n",
        "\n",
        "  val_pos_path = os.path.join(root_path, 'val', 'pos')\n",
        "  val_neg_path = os.path.join(root_path, 'val', 'neg')\n",
        "\n",
        "  test_pos_path = os.path.join(root_path, 'test_v2', 'pos')\n",
        "  test_neg_path = os.path.join(root_path, 'test_v2', 'neg')\n",
        "\n",
        "  copy_files(val_pos, val_pos_path)\n",
        "  copy_files(test_pos, test_pos_path)\n",
        "  copy_files(val_neg, val_neg_path)\n",
        "  copy_files(test_neg, test_neg_path)\n",
        "\n",
        "  return root_path + \"/val/\" , root_path + \"/test_v2/\"\n"
      ],
      "metadata": {
        "id": "ZIZ09jAr23HH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valPath_, testPath_ = val_test_split(testPath, 15000)"
      ],
      "metadata": {
        "id": "zse4FKx4uyd1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use `tf.data` to create an efficient dataset for each set.\n"
      ],
      "metadata": {
        "id": "zxb2QN2-23VT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_filepath_ds = tf.data.Dataset.list_files(trainPath + \"*/*\", seed = 42)\n",
        "val_filepath_ds = tf.data.Dataset.list_files(valPath_ + \"*/*\", seed = 42)\n",
        "test_filepath_ds = tf.data.Dataset.list_files(testPath_ + \"*/*\", seed = 42)\n"
      ],
      "metadata": {
        "id": "mnGMj5eLp3bn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def instance_label(file_path):\n",
        "  label = 0 if tf.strings.split(file_path, sep = '/')[4] == 'neg' else 1\n",
        "  return  tf.data.Dataset.from_tensors(label)"
      ],
      "metadata": {
        "id": "cwfYUIUBGV5Q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = train_filepath_ds.interleave(\n",
        "    lambda x: tf.data.Dataset.zip((tf.data.TextLineDataset(x), instance_label(x))),\n",
        "    cycle_length = 5,\n",
        "    num_parallel_calls = tf.data.experimental.AUTOTUNE\n",
        "  )\n",
        "\n",
        "val_ds = val_filepath_ds.interleave(\n",
        "    lambda x: tf.data.Dataset.zip((tf.data.TextLineDataset(x), instance_label(x))),\n",
        "    cycle_length = 5,\n",
        "    num_parallel_calls = tf.data.experimental.AUTOTUNE\n",
        "  )\n",
        "\n",
        "test_ds = test_filepath_ds.interleave(\n",
        "    lambda x: tf.data.Dataset.zip((tf.data.TextLineDataset(x), instance_label(x))),\n",
        "    cycle_length = 5,\n",
        "    num_parallel_calls = tf.data.experimental.AUTOTUNE\n",
        "  )"
      ],
      "metadata": {
        "id": "h3JXQ31-EfI3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for text, label in train_ds.take(1):\n",
        "  print(text)\n",
        "  print(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DvOZbZuQ_9D",
        "outputId": "4ce2d8c6-f4e4-4523-e164-bd615ba5efcd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b\"John Rivers' life as an architect and family man has taken a turn for the worst when his wife has disappeared and has been concluded dead after a freakish accident that involved changing a tyre on her car. During the days she has been missing, he confronts a man that's been following and he tells him that his been in contact with his dead wife from the other-side through E.V.P - Electronic Voice Phenomenon. Naturally he doesn't believe it but then hear gets weird phone calls from her phone and so he contacts the man to find out more about E.V.P. Soon enough John is hooked onto it, but something supernatural doesn't like him interfering with the dead, as now other then contacting his wife, the white noise is foretelling events before they happen.<br /><br />Since this DVD has been sitting on my shelf for a while now, I thought I better get around to watching it since it wasn't my copy. But then again I don't think the owners were in a hurry to get it back, as they haven't question me about it. Oh well. So I decided to give it a play, as I was in an undemanding mood. After hearing and reading all the bad press on it, I wasn't expecting anything remotely good, but I was kept entertained for 90 minutes. Well, more so the 60 minutes, as the last half-an-hour was pretty much a blur of confusion. The film is nowhere as good as it could have been, but the time breezed by quick enough even though it's a rather tepid supernatural thriller. I thought it wasn't all a waste. The first hour I found some effective sequences rather interesting and there's a spooky awe generated with a slow progression of subtle stillness and tragedy that haunts you, but sadly that comes to a crashing halt later on in the film. That's when the predictably forced jump scares come into their own and somehow it just doesn't fit in with the context. It becomes rather hectic, loud and very muddled with its MTV style editing and kinetic camera-work that gets to close into the action. I couldn't understand what was going on within choppy and abrupt climax. The whole explanation how everything fits into the bigger picture is pure hokey. It's a very unsatisfying conclusion because it goes for something big, but hits rock bottom. I thought they did fine job up until that point with the lighting and showy camera-work. Other then the distinctively stark lighting, the score kept this flick atmospherically gloomy. All of it is very slickly done with its glossed up and fancy hardware, which makes it come across as very sterile and empty.<br /><br />You can easily see that the film's heart is in the technical components and not in expanding the characters and story. There's just no connection and lasting sentiment within this flimsy material. After a while, it just tries too hard to convince you that it falls into manipulative thrills and popping in many blood-curdling stuff from beyond the grave. It just got rather repetitious watching someone watch a fuzzy TV screen after while. The E.V.P machine was the star on the show. Well, it did have more impact than the limp performances. Michael Keaton is more than capable actor, but lately his disappeared off the map and here he provides a modest performance as the dangerously obsessed John Rivers. He really deserves much better, though. Everyone else is pretty brittle and forgettable. Not because of the performances, but of the lack of depth in their characters. This clunker wasn't bad to begin with, but it does go pear shape by falling away drastically.<br /><br />I wouldn't care to see it again and I wouldn't recommend to anyone, unless you got a interest for the subject matter and enjoy the recent crop of Hollywood produced horror/thrillers. It's just a damn shame that this over-produced flick couldn't put it together successfully, as it had promise in its idea and a more than decent cast on hand. I didn't hate it, but what a disappointment.\", shape=(), dtype=string)\n",
            "tf.Tensor(0, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a binary classification model, using a `TextVectorization` layer to pre- process each review. If the `TextVectorization` layer is not yet available (or if you like a challenge), try to create your own custom preprocessing layer: you can use the functions in the `tf.strings` package, for example `lower()` to make everything lowercase, `regex_replace()` to replace punctuation with spaces, and `split()` to split words on spaces. You should use a lookup table to output word indices, which must be prepared in the `adapt()` method."
      ],
      "metadata": {
        "id": "HOAb9hbg25aF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTextVectorization(tf.keras.Layer):\n",
        "\n",
        "  def __init__(self,\n",
        "               max_tokens = None,\n",
        "               standardization = \"lower_and_strip_punctuation\",\n",
        "               split = True,\n",
        "               num_oov_buckets = 2,\n",
        "               max_len = None,\n",
        "               paddings = None,\n",
        "               **kwargs):\n",
        "\n",
        "    super(CustomTextVectorization, self).__init__(**kwargs)\n",
        "    self.max_tokens = max_tokens\n",
        "    self.standardization = standardization\n",
        "    self.split = split\n",
        "    self.num_oov_buckets = num_oov_buckets\n",
        "    self.max_len = max_len\n",
        "    self.paddings = paddings\n",
        "    self.init = None\n",
        "    self.table = None\n",
        "\n",
        "\n",
        "  def _standardize(self, text):\n",
        "    ops = self.standardization.split('_')\n",
        "    if 'lower' in ops:\n",
        "      text = tf.strings.lower(text)\n",
        "    if 'punctuation' in ops:\n",
        "      text = tf.strings.regex_replace(text, r'[\\p{P}]', '')\n",
        "\n",
        "    text = tf.strings.regex_replace(text, r'/<br\\s*/?/>', '')\n",
        "    return text\n",
        "\n",
        "  def _split(self, text):\n",
        "    if self.split:\n",
        "      return tf.strings.split(text)\n",
        "    return [text]\n",
        "\n",
        "  def adapt(self, data):\n",
        "    words = []\n",
        "    for text, label in data.take(10):\n",
        "      text = self._standardize(text)\n",
        "      splits = self._split(text)\n",
        "      for substring in splits:\n",
        "         words.append(substring)\n",
        "\n",
        "    keys, _ = tf.unique(words)\n",
        "    values = tf.constant(tf.cast(np.arange(tf.size(keys)), tf.int64))\n",
        "    self.init = tf.lookup.KeyValueTensorInitializer(keys, values)\n",
        "    self.table = tf.lookup.StaticVocabularyTable(self.init, self.num_oov_buckets)\n",
        "    return self\n",
        "\n",
        "  def call(self, item):\n",
        "    X = item\n",
        "    word_list = tf.strings.split(X)\n",
        "    word_vector = tf.map_fn(lambda x: self.table.lookup(x), word_list, dtype=tf.int64)\n",
        "    len_ = tf.shape(word_vector)[1]\n",
        "    print(tf.shape(word_vector), len_, word_vector.shape)\n",
        "    if len_ < self.max_len:\n",
        "      word_vector = tf.pad(\n",
        "          word_vector,\n",
        "          tf.constant([[0,self.max_len - len_.numpy()]]),\n",
        "          mode='CONSTANT',\n",
        "          constant_values=-1)\n",
        "\n",
        "    return word_vector[:self.max_len]\n",
        "\n",
        "  def compute_output_shape(self, input_shape):\n",
        "    return (input_shape[0], self.max_len)\n",
        "\n"
      ],
      "metadata": {
        "id": "IoMSv6Ri3ECm"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l = CustomTextVectorization(max_len = 100)\n",
        "l.adapt(train_ds)\n",
        "l(tf.constant(\"here is something\"))\n",
        "# l.compute_output_shape((1,))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLbtEW5v9pQd",
        "outputId": "32421617-0201-4e43-b44a-e6e90ede68c7"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(100,), dtype=int64, numpy=\n",
              "array([327,  74, 936,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
              "        -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
              "        -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
              "        -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
              "        -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
              "        -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
              "        -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
              "        -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1])>"
            ]
          },
          "metadata": {},
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.Input(shape = (), dtype = tf.string),\n",
        "        CustomTextVectorization(max_len = 100).adapt(train_ds),\n",
        "        tf.keras.layers.Dense(200,  activation = \"selu\", kernel_initializer=\"lecun_normal\"),\n",
        "        tf.keras.layers.Dense(200,  activation = \"selu\", kernel_initializer=\"lecun_normal\"),\n",
        "        tf.keras.layers.Dense(100,  activation = \"selu\", kernel_initializer=\"lecun_normal\"),\n",
        "        tf.keras.layers.Dense(10,  activation = \"selu\", kernel_initializer=\"lecun_normal\"),\n",
        "        tf.keras.layers.Dense(2, activation = 'softmax')\n",
        "    ]\n",
        ")\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "VXi05bjxoBxl",
        "outputId": "8245c71d-8112-4cc7-b5d7-b02b29527703"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_13\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_13\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ custom_text_vectorization_65         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mCustomTextVectorization\u001b[0m)            │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_65 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)                 │          \u001b[38;5;34m20,200\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_66 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)                 │          \u001b[38;5;34m40,200\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_67 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │          \u001b[38;5;34m20,100\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_68 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │           \u001b[38;5;34m1,010\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_69 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │              \u001b[38;5;34m22\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ custom_text_vectorization_65         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">CustomTextVectorization</span>)            │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_65 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">20,200</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_66 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">40,200</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_67 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">20,100</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_68 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_69 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m81,532\u001b[0m (318.48 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">81,532</span> (318.48 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m81,532\u001b[0m (318.48 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">81,532</span> (318.48 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer = \"Nadam\",\n",
        "    loss = \"binary_crossentropy\",\n",
        "    metrics = [\"accuracy\"]\n",
        ")"
      ],
      "metadata": {
        "id": "BfaiWEa4-rm9"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_batch = 32\n",
        "n_epoch = 100\n",
        "escb = tf.keras.callbacks.EarlyStopping(patience = 20)\n",
        "batched_train_ds = train_ds.batch(n_batch)\n",
        "batched_val_ds = val_ds.batch(n_batch)\n",
        "\n",
        "model.fit(batched_train_ds, validation_data = batched_val_ds, epochs = n_epoch, callbacks = [escb])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "C4MxH2re--eh",
        "outputId": "c5c471dc-9206-4b3c-ba38-468fbbc161e2"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "Tensor(\"sequential_13_1/custom_text_vectorization_65_1/Shape_1:0\", shape=(2,), dtype=int32) Tensor(\"sequential_13_1/custom_text_vectorization_65_1/strided_slice:0\", shape=(), dtype=int32) (None, None)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OperatorNotAllowedInGraphError",
          "evalue": "Exception encountered when calling CustomTextVectorization.call().\n\n\u001b[1mUsing a symbolic `tf.Tensor` as a Python `bool` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\u001b[0m\n\nArguments received by CustomTextVectorization.call():\n  • item=tf.Tensor(shape=(None,), dtype=string)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-198-eea2647df48e>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbatched_val_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_train_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatched_val_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mescb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-195-78ec562b6a52>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mlen_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mlen_\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m       word_vector = tf.pad(\n\u001b[1;32m     60\u001b[0m           \u001b[0mword_vector\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOperatorNotAllowedInGraphError\u001b[0m: Exception encountered when calling CustomTextVectorization.call().\n\n\u001b[1mUsing a symbolic `tf.Tensor` as a Python `bool` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\u001b[0m\n\nArguments received by CustomTextVectorization.call():\n  • item=tf.Tensor(shape=(None,), dtype=string)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Add an `Embedding` layer and compute the mean embedding for each review, multiplied by the square root of the number of words (see Chapter 16). This rescaled mean embedding can then be passed to the rest of your model.\n"
      ],
      "metadata": {
        "id": "7IpGr8u53ETE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s7TYEq0Z3HLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model and see what accuracy you get. Try to optimize your pipelines\n",
        "to make training as fast as possible."
      ],
      "metadata": {
        "id": "BrdKNyu53HgA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z6WWNJ363LDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use TFDS to load the same dataset more easily: `tfds.load(\"imdb_reviews\")`."
      ],
      "metadata": {
        "id": "hIg4xHeO3LcW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oV-AfpQ83t56"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}